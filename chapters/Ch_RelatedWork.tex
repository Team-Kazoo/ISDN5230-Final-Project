\chapter{Related Work}\label{chap:related}

\section{Pitch Detection Algorithms}\label{sec:pitch_detection}

Fundamental frequency estimation, commonly termed pitch detection, has remained an active research area for over five decades despite the apparent simplicity of the underlying problem. The challenge arises from the complex acoustic structure of natural sounds, where the fundamental frequency may be absent or weak relative to harmonics, noise may obscure periodic structure, and rapid pitch variations may violate the stationarity assumptions underlying many analysis techniques. Our review focuses on approaches relevant to monophonic vocal input processing with real-time latency constraints.

The autocorrelation method, formalized in the signal processing literature during the 1960s, computes the similarity between a signal and time-shifted versions of itself, with the lag corresponding to maximum similarity indicating the fundamental period. While computationally straightforward and robust to missing fundamentals, autocorrelation suffers from systematic errors including octave confusion arising from strong harmonics and sensitivity to formant structure that can bias estimates toward vocal tract resonances rather than true fundamental frequency. These limitations motivated development of more sophisticated approaches that incorporate domain knowledge about acoustic signal structure.

The YIN algorithm introduced by de Cheveign√© and Kawahara~\cite{decheveigne2002yin} represented a significant advance through systematic analysis and mitigation of autocorrelation failure modes. YIN replaces autocorrelation with a squared difference function and applies cumulative mean normalization to eliminate the bias toward lower frequencies inherent in standard autocorrelation. Additionally, YIN employs parabolic interpolation to achieve sub-sample period estimation, improving frequency resolution beyond the limit imposed by sample rate. Empirical evaluation demonstrated error rates approximately three times lower than competing methods available at the time, establishing YIN as a preferred choice for applications requiring robust monophonic pitch detection.

The emergence of deep learning has motivated investigation of neural approaches to pitch detection. The CREPE system introduced by Kim et al.~\cite{kim2018crepe} applies convolutional neural networks trained on large annotated datasets to achieve state-of-the-art accuracy, particularly in challenging conditions including background noise and polyphonic interference. However, neural pitch detectors require model inference that introduces both computational latency and memory overhead that may prove problematic for resource-constrained deployment scenarios. Recent work on model quantization and knowledge distillation has reduced these requirements, with quantized CREPE variants achieving approximately 2ms inference time on modern CPUs, approaching the latency characteristics required for real-time applications.

The OneBitPitch algorithm proposed by Korepanov et al.~\cite{korepanov2023onebitpitch} represents an alternative approach prioritizing computational efficiency through extreme signal quantization. By processing single-bit representations of the audio signal, OneBitPitch achieves approximately 9x speedup compared to YIN while maintaining acceptable accuracy for many applications. However, the aggressive quantization sacrifices information that proves valuable for harmonically rich signals typical of human voice production, limiting applicability to our target domain.

For the vocal synthesis application addressed in this work, the YIN algorithm provides an optimal balance between accuracy and computational efficiency. The deterministic nature of classical DSP algorithms ensures consistent latency across frames, a property essential for maintaining the stable temporal relationship between vocal input and synthesized output that performers require for expressive control. Furthermore, the computational requirements of YIN fall well within the processing budget available in modern AudioWorklet implementations, leaving substantial headroom for additional processing stages including spectral feature extraction and signal smoothing.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/fig1_pitch_detection.png}
  \caption{Comparative analysis of pitch detection algorithms across accuracy, latency, and computational requirements. The scatter plot presents latency on the x-axis (logarithmic scale, 0.01--100ms) and accuracy on the y-axis (85--100\%), with point size indicating computational cost. YIN occupies an advantageous position in the low-latency, high-accuracy region while maintaining moderate computational requirements, justifying its selection for real-time vocal processing applications.}
  \label{fig:pitch_detection_comparison}
\end{figure}

\section{Neural Audio Synthesis}\label{sec:neural_synthesis}

The intersection of machine learning and audio synthesis has generated remarkable advances over the past decade, culminating in the 2023--2024 emergence of commercial text-to-music platforms that have fundamentally transformed public expectations for AI-generated audio. Our review traces this evolution from early symbolic music generation through contemporary audio-domain synthesis and commercial deployment, contextualizing our technical approach within this rapidly advancing landscape.

Google Magenta's research program initiated systematic investigation of neural network applications to music generation beginning in 2016. The MelodyRNN system demonstrated that Long Short-Term Memory (LSTM) networks trained on large corpora of symbolic music data could generate melodic continuations exhibiting learned stylistic characteristics. Performance RNN~\cite{simon2017performancernn} extended this approach by incorporating expressive timing and dynamics, using a temporal resolution of 10 milliseconds to capture the subtle timing variations that distinguish mechanical playback from human performance. These symbolic approaches operate on discrete note representations rather than raw audio, enabling generation at computational costs compatible with real-time applications while sacrificing the timbral richness of audio-domain methods. Critically for our application, MusicRNN's computational efficiency---requiring only 250--500ms for sequence generation---enables deployment within browser environments where GPU acceleration cannot be assumed.

The Music Transformer architecture introduced by Huang et al.~\cite{huang2019musictransformer} applied self-attention mechanisms to symbolic music generation, demonstrating improved capacity for maintaining long-term structural coherence compared to recurrent approaches. This architectural innovation presaged the transformer-based designs that would later power commercial systems. However, the quadratic complexity of self-attention with respect to sequence length imposes computational constraints that currently limit real-time applicability for extended musical contexts without specialized hardware acceleration.

Audio-domain neural synthesis emerged as a distinct research direction with the introduction of WaveNet by van den Oord et al.~\cite{oord2016wavenet} in 2016, demonstrating that autoregressive neural networks could generate audio samples of remarkable quality. However, the sequential sample-by-sample generation process proved computationally prohibitive for real-time synthesis. The field subsequently bifurcated into two trajectories: high-quality offline generation pursuing ever-greater fidelity, and real-time systems accepting architectural constraints to achieve interactive latencies.

The offline generation trajectory has achieved remarkable commercial success. Google's MusicLM, published in January 2023~\cite{agostinelli2023musiclm}, demonstrated that hierarchical sequence-to-sequence modeling combining MuLan text-music embeddings with AudioLM's discrete audio tokenization could generate high-fidelity music remaining consistent over several minutes. Meta's MusicGen, released as open-source in June 2023~\cite{copet2023musicgen}, employed a single-stage transformer language model over compressed audio representations from EnCodec, enabling community customization that accelerated subsequent research. Stability AI's Stable Audio, first released in September 2023, applied latent diffusion techniques with timing conditioning to achieve rapid generation---rendering 95 seconds of stereo audio in under one second on an NVIDIA A100 GPU. The April 2024 release of Stable Audio 2.0~\cite{evans2024stablediffusion} introduced diffusion transformers (DiT) for improved long-form coherence and audio-to-audio transformation capabilities.

The commercial platforms Suno and Udio represent the current state-of-the-art in production-quality music generation. Technical analysis suggests these systems employ hybrid architectures: large language models parse user prompts into structured musical intent vectors specifying tempo, key, and instrumentation, which then condition diffusion-based audio generators potentially utilizing proprietary compression models trained on extensive licensed or unlicensed musical corpora. Suno's V4 model, released November 2024, and Udio's continuously evolving system demonstrate that the research advances of preceding years have translated into commercially viable products, albeit ones requiring substantial server-side GPU computation incompatible with client-side deployment.

The real-time trajectory has pursued different architectural strategies to achieve interactive latencies. Differentiable Digital Signal Processing (DDSP) introduced by Engel et al.~\cite{engel2020ddsp} represented a paradigm shift by combining interpretable signal processing primitives with neural network control. Rather than generating audio samples directly, DDSP models predict parameters for traditional synthesis algorithms including harmonic oscillator banks and filtered noise generators. This architectural choice dramatically reduces the dimensionality of the generation problem while maintaining physical interpretability, enabling real-time timbre transfer for monophonic signals using modest computational resources. The DDSP-VST plugin demonstrates that this approach can deliver professional-quality neural synthesis in standard digital audio workstation environments.

The Realtime Audio Variational autoEncoder (RAVE) developed by Caillon and Esling~\cite{caillon2021rave} achieves a different tradeoff by employing a multi-band representation that enables efficient parallel generation. RAVE achieves 25x real-time synthesis at 48kHz sample rate on CPU, generating any type of audio including polyphonic music and environmental sounds rather than being restricted to monophonic pitched content. Most recently, the BRAVE variant~\cite{caspe2025brave} achieves $\pm$3ms latency with only 4.9 million parameters, approaching the latency floor imposed by audio buffering rather than computation.

Emerging research on ``live music models'' seeks to combine the quality of generative approaches with the responsiveness required for performance. These systems produce continuous music streams with synchronized user control, though current implementations exhibit latencies of several seconds that preclude the tight feedback loop characteristic of traditional instrumental performance. The ReaLJam system~\cite{wu2025realjam} explores reinforcement learning-tuned transformers for human-AI jamming, though the fundamental tradeoff between model capacity and inference latency remains a defining constraint.

Our system navigates this landscape by strategically combining classical DSP for latency-critical pitch detection with neural sequence generation for musically supportive but non-time-critical accompaniment. While our synthesis cannot approach the timbral richness of DDSP or RAVE---which would require model weights exceeding practical browser download sizes and inference capabilities exceeding JavaScript performance---the MusicRNN-based accompaniment provides genuine AI-augmented musical interaction within browser constraints. This hybrid architecture demonstrates that meaningful AI musical capability can be delivered accessibly without the GPU clusters powering Suno and Udio, achieving a different but valuable point in the design space of AI music systems.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/fig2_timeline.jpg}
  \caption{Timeline visualization of neural audio synthesis development from 2016--2025, illustrating the progression from symbolic generation (MelodyRNN, Performance RNN, Music Transformer) through audio-domain approaches (WaveNet, DDSP, RAVE, Stable Audio) to commercial platforms (MusicLM, MusicGen, Suno, Udio). The figure employs a bifurcated structure showing the offline/high-quality trajectory (upper) and real-time/interactive trajectory (lower), with our system positioned at the intersection of browser accessibility and real-time interaction.}
  \label{fig:neural_synthesis_timeline}
\end{figure}

\section{Web Audio Processing}\label{sec:web_audio}

The maturation of web audio capabilities represents a critical enabling factor for browser-based music applications, transforming what was once a platform suitable only for simple sound playback into an environment capable of supporting sophisticated real-time audio processing. Our review traces this evolution from early HTML5 Audio elements through the modern AudioWorklet API that enables our implementation.

The HTML5 Audio element introduced during the late 2000s provided basic audio playback capabilities but offered no programmatic access to audio samples, precluding any custom processing or analysis. The Web Audio API specification developed by the W3C Audio Working Group~\cite{w3c2021webaudio} beginning in 2011 addressed this limitation by providing a comprehensive graph-based model for audio routing and processing. The API introduces AudioNode primitives that can be connected into arbitrary processing topologies, enabling applications ranging from simple gain adjustment through complex spatial audio rendering.

The ScriptProcessorNode included in early Web Audio API implementations provided the first mechanism for custom audio processing through JavaScript callbacks. However, this approach suffered from fundamental architectural limitations arising from its execution on the main JavaScript thread. Garbage collection pauses, UI rendering, and other main-thread activities could interrupt audio callback execution, producing audible glitches that severely degraded user experience. Furthermore, the minimum buffer size of 2048 samples imposed by many implementations---representing approximately 46 milliseconds at 44.1kHz sample rate---introduced latency incompatible with real-time interactive applications.

The AudioWorklet interface introduced in the Web Audio API 1.0 specification and implemented by major browsers beginning in 2018 addressed these limitations through architectural redesign. AudioWorklet processing executes on a dedicated real-time audio thread, isolated from main-thread activities that previously caused processing interruptions. This thread operates at elevated system priority, receiving preferential scheduling treatment that ensures audio callbacks complete within their allocated time budgets. Buffer sizes as small as 128 samples---approximately 2.9 milliseconds---become practical, enabling the low-latency processing required for interactive musical applications.

Performance analysis conducted by Mozilla Corporation~\cite{mozilla2020audioworklet} in conjunction with their 2020 Firefox AudioWorklet implementation provides empirical characterization of achievable performance. Their measurements demonstrate consistent sub-3ms processing latency for moderately complex audio graphs, with the dedicated audio thread maintaining stable timing even during intensive main-thread JavaScript execution. Chrome's subsequent introduction of ``Output Buffer Bypass'' further reduces latency by eliminating a buffering stage from the audio output path, approaching the theoretical minimum imposed by audio hardware characteristics.

Recent W3C Audio Working Group discussions focus on continued latency reduction and improved measurement capabilities. Proposals under consideration include exposing high-resolution timestamps within AudioWorklet scope to enable precise latency characterization, and mechanisms for AudioWorkletProcessor to report its intrinsic latency for incorporation into system-wide latency calculations. These ongoing standardization efforts suggest that browser audio capabilities will continue advancing toward parity with native audio processing environments.

Our implementation leverages AudioWorklet capabilities to achieve latency characteristics previously associated only with native code execution. By confining computationally intensive pitch detection and spectral analysis to the isolated audio thread while managing synthesis and visualization on the main thread, we exploit the architectural separation that AudioWorklet provides while maintaining the convenience and accessibility of browser-based deployment.

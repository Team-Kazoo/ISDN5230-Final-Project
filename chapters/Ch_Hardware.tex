\chapter{Hardware Deployment}\label{chap:hardware}

\section{Embedded Platform Selection}\label{sec:platform}

While the browser-based implementation described in previous chapters demonstrates the feasibility of real-time vocal synthesis on standard computing devices, deployment on embedded hardware offers significant advantages for dedicated musical instrument applications. Embedded deployment eliminates dependency on general-purpose computers, enables standalone operation without network connectivity, and provides consistent performance characteristics unaffected by background processes or browser updates. This chapter describes our deployment of the Mambo Whistle system on the Raspberry Pi 5 platform, demonstrating that the computational requirements of our hybrid DSP-neural architecture can be satisfied by modern single-board computers.

The Raspberry Pi 5, released in October 2023, represents a substantial advancement over previous generations in the Raspberry Pi family. The platform features a Broadcom BCM2712 system-on-chip incorporating a quad-core ARM Cortex-A76 processor operating at 2.4GHz, representing the first use of this high-performance core design in the Raspberry Pi lineup. The Cortex-A76 microarchitecture delivers approximately three times the single-threaded performance of the Cortex-A72 cores used in the Raspberry Pi 4, with particular improvements in floating-point throughput critical for audio signal processing workloads. The platform provides 8GB of LPDDR4X-4267 memory in our configuration, ensuring adequate capacity for neural model weights and audio buffers without memory pressure.

Several characteristics of the Raspberry Pi 5 proved particularly advantageous for audio processing applications. The platform includes a dedicated RP1 I/O controller that offloads peripheral management from the main processor, reducing interrupt latency and improving real-time performance predictability. The dual-band 802.11ac wireless and Bluetooth 5.0 connectivity enable wireless audio output to compatible speakers or headphones. The USB 3.0 ports operating at 5Gbps provide adequate bandwidth for professional audio interfaces, while the 40-pin GPIO header enables direct integration with custom hardware including additional sensors or display modules.

\section{System Architecture for Embedded Deployment}\label{sec:embedded_arch}

The embedded deployment architecture adapts the browser-based implementation to native execution while preserving the core algorithmic components. Rather than executing within a web browser environment, the embedded version runs as a native Linux application leveraging the ALSA (Advanced Linux Sound Architecture) audio subsystem for low-latency audio capture and playback.

The audio capture pipeline interfaces directly with USB audio devices through ALSA, bypassing the additional abstraction layers present in browser audio APIs. We employ the USB Audio Class (UAC) protocol supported natively by the Linux kernel, enabling plug-and-play compatibility with standard USB microphones without custom driver development. The capture configuration specifies 44.1kHz sample rate with 16-bit signed integer samples, matching the parameters used in our browser implementation to ensure algorithmic equivalence.

Audio processing executes in a dedicated real-time thread configured with SCHED\_FIFO scheduling policy and elevated priority, analogous to the AudioWorklet isolation in the browser implementation. The Linux kernel's PREEMPT\_RT patch set, installed on our deployment image, reduces worst-case interrupt latency to under 100 microseconds, ensuring that audio callbacks complete within their allocated time budgets even under system load. Process isolation through CPU affinity assignment dedicates one Cortex-A76 core exclusively to audio processing, preventing interference from system services or neural inference workloads.

The YIN pitch detection and FFT spectral analysis implementations port directly from JavaScript to C++ with minimal modification, benefiting from the language's deterministic memory management and absence of garbage collection pauses. The FFTW library (Fastest Fourier Transform in the West) replaces our custom FFT implementation, providing highly optimized transforms that exploit the ARM NEON SIMD instructions available on the Cortex-A76 cores. Benchmarking confirms that the C++ implementation completes pitch detection in approximately 150 microseconds per frame, a 3x improvement over the JavaScript version executing in browser environments.

Neural inference for harmonic accompaniment generation employs TensorFlow Lite, the embedded-optimized variant of the TensorFlow framework. The MusicRNN model weights convert to TensorFlow Lite format through the standard conversion pipeline, with INT8 quantization reducing model size from 5MB to 1.2MB while maintaining acceptable generation quality. The TensorFlow Lite interpreter executes on a separate CPU core from audio processing, with inference results communicated through a lock-free ring buffer to prevent synchronization overhead from affecting audio latency.

\section{Audio Hardware Configuration}\label{sec:audio_hardware}

The selection of appropriate audio input and output hardware significantly impacts achievable latency and audio quality in embedded deployments. Consumer USB microphones designed for video conferencing typically introduce substantial latency through internal buffering optimized for network transmission rather than real-time processing. We evaluated several audio interface options to identify configurations meeting our latency requirements.

Our primary deployment configuration employs a USB audio interface supporting USB Audio Class 2.0, which enables sample-accurate synchronization and buffer sizes as small as 32 samples. The audio interface provides balanced XLR microphone input with 48V phantom power for condenser microphones, though the system operates satisfactorily with dynamic microphones or electret capsules for cost-sensitive deployments. The interface's hardware monitoring capability enables zero-latency input monitoring independent of the processing pipeline, valuable for performers requiring immediate auditory feedback.

ALSA configuration through the \texttt{.asoundrc} file specifies hardware parameters optimized for low latency operation. The period size of 64 samples with 3 periods yields a theoretical minimum latency of approximately 4.4 milliseconds at 44.1kHz sample rate, though practical measurements indicate 8--12 milliseconds end-to-end latency depending on USB scheduling alignment. Buffer underrun prevention employs a watchdog thread that monitors ALSA buffer fill levels and triggers preemptive buffer stuffing when levels fall below safety thresholds.

Audio output routes through the same USB interface to maintain clock synchronization between capture and playback, eliminating drift that would otherwise accumulate when using independent clock domains. The Raspberry Pi's onboard PWM audio output, while functional for testing, exhibits excessive noise floor and insufficient sample rate accuracy for musical applications, motivating the use of external audio interfaces for deployment configurations.

\section{Software Environment and Dependencies}\label{sec:software}

The embedded deployment builds upon Raspberry Pi OS (64-bit), the official Debian-based Linux distribution optimized for Raspberry Pi hardware. The 64-bit variant enables access to the full 8GB memory capacity and provides improved performance for 64-bit optimized libraries including TensorFlow Lite. The PREEMPT\_RT kernel patch applied to the standard kernel image enables the real-time scheduling guarantees required for consistent audio processing latency.

Software dependencies install through a combination of system packages and compiled-from-source libraries. The build environment includes GCC 12 with ARM-specific optimization flags enabling auto-vectorization for NEON SIMD instructions. Key dependencies include:

\begin{itemize}
    \item \textbf{ALSA development libraries} (libasound2-dev): Audio capture and playback interface
    \item \textbf{FFTW3} (libfftw3-dev): Optimized Fast Fourier Transform implementation
    \item \textbf{TensorFlow Lite}: Neural network inference runtime with ARM optimizations
    \item \textbf{libsndfile}: Audio file I/O for configuration and calibration sounds
    \item \textbf{RtMidi}: MIDI output for integration with external synthesizers
\end{itemize}

The application packages as a systemd service enabling automatic startup on boot, headless operation without display attachment, and automatic restart on unexpected termination. Service configuration specifies resource limits including memory locking permissions required for real-time audio operation and CPU affinity constraints ensuring consistent core assignment across restarts.

\section{Performance Characterization}\label{sec:embedded_performance}

Systematic performance characterization validates that the embedded deployment meets real-time requirements across representative operating conditions. Measurements employ a loopback configuration where synthesized output routes back to the audio input, enabling precise round-trip latency measurement through correlation analysis.

End-to-end audio latency from microphone input to speaker output measures 23--28 milliseconds under typical conditions, substantially lower than the 50--60 milliseconds achieved in browser deployments. This improvement derives from elimination of browser audio buffering, direct ALSA hardware access, and optimized native code execution. The achieved latency falls well within the 100-millisecond threshold for perceived simultaneity and approaches the range where trained musicians report negligible perceptual delay.

CPU utilization during continuous operation averages 18\% of a single Cortex-A76 core for the audio processing thread, with brief spikes to 35\% during neural inference execution on a second core. The substantial headroom below 100\% utilization ensures that processing completes within each audio frame period without buffer underruns. Memory utilization remains stable at approximately 180MB including TensorFlow Lite runtime and model weights, well within the 8GB available capacity.

Thermal performance under sustained operation maintains processor temperature below 65°C with the official Raspberry Pi active cooler, avoiding thermal throttling that would compromise real-time guarantees. Passive cooling configurations prove inadequate for continuous operation, with temperatures exceeding 80°C and triggering frequency reduction within 10 minutes of sustained audio processing.

Power consumption measurements indicate approximately 6.5W average draw during active audio processing, compatible with portable battery operation using standard USB-C power delivery sources. A 20,000mAh USB-C battery pack provides approximately 12 hours of continuous operation, enabling mobile deployment for performances or field recording applications.

\section{Deployment Procedure}\label{sec:deployment}

The deployment procedure transforms a stock Raspberry Pi 5 into a functional Mambo Whistle embedded instrument through a reproducible sequence of configuration and installation steps. The procedure assumes starting from a fresh Raspberry Pi OS installation on a high-endurance microSD card or USB solid-state drive.

Initial system configuration establishes prerequisites for real-time audio operation. The \texttt{raspi-config} utility enables necessary interfaces and configures boot parameters. Memory split configuration allocates minimum GPU memory (16MB) to maximize RAM available for audio processing. The real-time kernel installation involves downloading the PREEMPT\_RT patched kernel package and updating the boot configuration to select it.

User permissions configuration grants the application user membership in the \texttt{audio} group for ALSA device access and configures PAM limits for memory locking and real-time scheduling priority. The \texttt{/etc/security/limits.conf} file specifies:

\begin{verbatim}
@audio   -  rtprio     95
@audio   -  memlock    unlimited
\end{verbatim}

Application installation proceeds through cloning the project repository, installing build dependencies, and executing the CMake-based build system. The build process requires approximately 8 minutes on the Raspberry Pi 5, with parallel compilation across all four cores. Installation copies the compiled binary and configuration files to standard system locations, with the systemd service file enabling automatic startup.

Calibration procedures executed on first run characterize the specific audio hardware configuration and optimize processing parameters. The calibration sequence measures round-trip latency, adjusts input gain for optimal signal-to-noise ratio, and verifies pitch detection accuracy against reference tones. Calibration results persist in a configuration file loaded on subsequent startups.

\section{User Interface for Embedded Operation}\label{sec:embedded_ui}

Embedded operation without attached display necessitates alternative user interface approaches for configuration and status monitoring. Our implementation provides multiple interface modalities accommodating different deployment scenarios.

Physical controls through GPIO-connected hardware enable essential functions without network connectivity. A rotary encoder with push-button provides instrument preset selection and parameter adjustment, with detent positions corresponding to discrete instrument voices. LED indicators communicate system status including power, audio activity, pitch detection confidence, and error conditions through color and blink pattern encoding.

Network-based remote control exposes a WebSocket API enabling configuration and monitoring from smartphones, tablets, or computers on the same network. A companion mobile application developed with React Native provides touch-friendly interfaces for instrument selection, effect parameter adjustment, and performance visualization. The WebSocket connection also enables real-time streaming of pitch detection data for external visualization or recording applications.

SSH access provides full administrative control for advanced configuration, log inspection, and troubleshooting. The \texttt{journalctl} command retrieves application logs captured by the systemd journal, facilitating remote diagnosis of operational issues.

\section{Limitations and Future Hardware Directions}\label{sec:hardware_limitations}

The current embedded deployment, while functional, exhibits limitations that future iterations could address. The reliance on USB audio interfaces introduces a cost component and physical bulk that integrated audio solutions could eliminate. The Raspberry Pi 5's lack of integrated high-quality audio codec necessitates external hardware, unlike some competing single-board computers that include suitable onboard audio.

Latency, while substantially improved over browser deployment, remains above the threshold where some performers report perceiving delay. Further reduction would require custom audio hardware with DMA-based sample transfer, bypassing the USB stack overhead that contributes several milliseconds to the current latency budget.

Future development directions include exploration of FPGA-based acceleration for pitch detection, enabling sub-millisecond analysis latency through parallel hardware implementation of the YIN algorithm. The Raspberry Pi 5's PCI Express interface could accommodate FPGA accelerator cards, though this approach would substantially increase system cost and complexity. Alternative single-board computers with integrated NPU (Neural Processing Unit) accelerators could enable more sophisticated neural synthesis models while maintaining real-time performance, potentially approaching the audio quality achievable with DDSP or RAVE architectures.

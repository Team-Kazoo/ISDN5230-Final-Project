\chapter{Introduction}\label{chap:introduction}

\section{Background and Motivation}\label{sec:background}

The aspiration to transform the human voice into instrumental sounds has captivated musicians, engineers, and researchers for decades, representing one of the most intuitive yet technically demanding challenges in music technology. Unlike traditional musical instruments that require years of physical training to master, the human voice represents a universally accessible input modality that every individual can control with remarkable precision and expressiveness from birth. This fundamental observation motivates our investigation into voice-controlled synthesis systems that could potentially lower the barriers to musical expression and enable individuals without formal instrumental training to participate in music creation.

The landscape of AI-powered music technology has undergone revolutionary transformation in recent years, fundamentally reshaping expectations for what machines can accomplish in creative audio domains. The emergence of text-to-music generation platforms, most notably Suno AI and Udio, has demonstrated that end-to-end music creation from natural language descriptions has become commercially viable. Suno, valued at \$500 million following its May 2024 Series B funding, employs a hybrid architecture combining transformer-based language models with diffusion components to generate production-quality music from text prompts, while Udio, developed by former Google DeepMind researchers, applies spectrogram-domain diffusion techniques inspired by Stable Diffusion to achieve remarkable audio fidelity. These systems represent the culmination of research trajectories initiated by Google's MusicLM in January 2023~\cite{agostinelli2023musiclm}, which first demonstrated high-fidelity text-conditioned music generation through hierarchical sequence-to-sequence modeling at 24kHz sample rate. The subsequent release of Meta's open-source MusicGen in June 2023~\cite{copet2023musicgen} further accelerated progress by enabling community-driven development and customization. However, these generative systems operate fundamentally differently from the real-time interactive paradigm we address: they require seconds to minutes of computation to produce audio, precluding the immediate feedback loop essential for expressive musical performance.

The distinction between generative music AI and real-time musical interaction represents a critical axis along which our work is positioned. While Suno and Udio generate complete musical works from textual descriptions, and Stability AI's Stable Audio 2.0 produces coherent three-minute tracks through latent diffusion with timing conditioning~\cite{evans2024stablediffusion}, these systems cannot respond to performer input with the sub-100ms latency required for the sensation of instrumental control. Recent research on ``live music models'' has begun addressing this gap, with systems such as Magenta RealTime demonstrating continuous music generation with synchronized user control. However, these approaches typically require substantial GPU resources and exhibit latencies of several seconds between input and output, making them unsuitable for the tight feedback loop that characterizes traditional instrumental performance. Our work targets this underexplored intersection: real-time, low-latency vocal-to-instrument transformation with AI accompaniment, deployed accessibly through standard web browsers without specialized hardware requirements.

Traditional approaches to vocal-instrumental transformation have historically required substantial investments in specialized hardware, including dedicated MIDI wind controllers, electronic wind instruments such as the Yamaha WX series, or sophisticated pitch-to-MIDI converters that demand careful acoustic isolation and calibration. These solutions, while effective within their operational parameters, impose significant financial and technical barriers that limit accessibility to professional musicians and well-funded educational institutions. Moreover, the latency characteristics of many existing systems exceed the approximately 100ms threshold identified by Wessel and Wright~\cite{wessel2002problems} as the upper bound for perceived simultaneity in musical performance, fundamentally compromising the intimate connection between performer gesture and sonic result that characterizes expressive instrumental playing. The MIT Media Lab's recent work on AI-augmented musical instruments~\cite{sarkar2024aiaugmented} emphasizes that generative AI systems embedded within instruments must provide extensive control and responsiveness to real-time musical inputs---a design philosophy that directly informs our architectural decisions.

The emergence of neural audio synthesis techniques over the past five years has dramatically expanded the possibilities for audio transformation and generation. Pioneering work on Differentiable Digital Signal Processing (DDSP) by Engel et al.~\cite{engel2020ddsp} demonstrated that combining traditional signal processing primitives with learned neural components could achieve remarkable timbral quality while maintaining interpretability, with the DDSP-VST plugin enabling real-time timbre transfer in standard digital audio workstations. Subsequently, the Realtime Audio Variational autoEncoder (RAVE) introduced by Caillon and Esling~\cite{caillon2021rave} achieved 25x real-time synthesis at 48kHz sample rate on CPU, suggesting that neural approaches could eventually meet the stringent latency requirements of live performance. The recent BRAVE variant~\cite{caspe2025brave} achieves $\pm$3ms latency with only 4.9 million parameters while preserving timbre-transfer quality. However, deploying these sophisticated models within browser environments presents additional constraints: model weights must be transmitted over networks, inference must execute within JavaScript or WebAssembly runtime environments, and computational resources cannot be assumed to include GPU acceleration. These constraints motivate our hybrid architecture that strategically allocates neural computation to non-latency-critical tasks while relying on optimized classical algorithms for the time-sensitive pitch detection pipeline.

Concurrent with advances in neural audio synthesis, web browser capabilities have evolved substantially through standardization efforts led by the World Wide Web Consortium (W3C). The introduction of the AudioWorklet API in 2018 addressed fundamental architectural limitations of earlier browser audio processing by enabling computation on dedicated real-time threads, isolated from the garbage collection pauses and rendering overhead that characterize main-thread JavaScript execution. Performance analysis conducted by Mozilla Corporation~\cite{mozilla2020audioworklet} demonstrates that modern AudioWorklet implementations can process buffers as small as 128 samples---approximately 3 milliseconds at standard 44.1kHz sample rate---representing a dramatic improvement over the minimum 2048-sample buffers required by the deprecated ScriptProcessorNode interface. Chrome's ``Output Buffer Bypass'' feature further reduces latency by eliminating buffering stages from the audio output path. These developments, combined with emerging WebGPU capabilities for GPU-accelerated computation in browsers, suggest that the browser platform has matured sufficiently to support professional-grade audio applications that previously required native code execution.

The democratization potential of browser-based music technology extends beyond mere convenience, particularly when contrasted with the resource requirements of state-of-the-art music AI systems. While Suno reports that GPU compute exceeds payroll costs by several multiples, and professional AI music performance systems such as those used by artists like Holly Herndon require custom-built gaming PCs with high-performance GPUs, browser-based approaches can reach any user with a standard laptop and microphone. Browser applications eliminate installation barriers entirely, enabling immediate access on any device equipped with modern web browser capabilities. This characteristic proves particularly valuable in educational contexts where IT infrastructure limitations often impede the deployment of specialized music software. Furthermore, client-side processing architectures address the growing societal concern regarding audio surveillance by ensuring that sensitive vocal data never leaves the user's device---a privacy guarantee that cloud-based music generation services fundamentally cannot provide, and one that has gained particular relevance following the June 2024 Recording Industry Association of America lawsuit against Suno and Udio regarding training data practices.

\section{Problem Statement}\label{sec:problem}

This work investigates whether real-time, expressive vocal-to-instrument synthesis with sub-100ms latency can be achieved through accessible computing platforms while simultaneously providing AI-powered harmonic accompaniment that enhances the musical experience. We address this question through two complementary deployment strategies: browser-based implementation for universal accessibility, and embedded hardware deployment on the Raspberry Pi 5 platform for professional-grade latency performance. This dual-track approach encompasses several interrelated technical challenges that must be addressed systematically.

The Web Audio API, while providing powerful primitives for audio processing, introduces multiple buffering stages that accumulate latency throughout the signal path from microphone input to speaker output. Each buffer represents a fundamental tradeoff between latency and computational headroom, as smaller buffers provide lower latency but leave less time for completing processing before the next buffer arrives. Our investigation must therefore identify buffer configurations and processing architectures that minimize cumulative latency while maintaining sufficient headroom for computationally intensive analysis algorithms.

JavaScript execution characteristics present additional challenges for real-time audio applications. The language's garbage collection mechanism can introduce unpredictable pauses that, if occurring during audio processing, manifest as audible discontinuities that severely degrade user experience. Furthermore, JavaScript's single-threaded execution model historically meant that computationally intensive operations would block UI responsiveness, creating an inherent tension between processing depth and interface fluidity. Our architecture must leverage modern browser threading capabilities to isolate time-critical audio processing from non-deterministic main-thread activities.

Algorithm selection for pitch detection presents a nuanced optimization problem that balances accuracy, computational cost, and latency characteristics. Neural pitch detectors such as CREPE~\cite{kim2018crepe} achieve superior accuracy in noisy conditions but require model inference that may introduce latency incompatible with real-time requirements. Classical algorithms including autocorrelation and the YIN algorithm~\cite{decheveigne2002yin} provide deterministic execution time but may sacrifice accuracy under adverse acoustic conditions. Our system must select and optimize algorithms appropriate for the target application domain of clean vocal input from consumer microphones.

Finally, integrating neural network inference for harmonic accompaniment generation requires careful architectural consideration to prevent machine learning computation from blocking time-critical audio processing. Neural sequence generation through models such as MusicRNN requires substantial computation that cannot complete within typical audio buffer periods, necessitating asynchronous execution strategies that decouple inference timing from audio synthesis.

For embedded deployment, additional challenges arise from the constraints of single-board computer platforms. The Raspberry Pi 5, while representing the most capable device in its product line, provides substantially less computational power than desktop systems, necessitating aggressive optimization of DSP algorithms. Real-time Linux kernel configuration, CPU core isolation, and memory management strategies must be carefully tuned to achieve consistent sub-30ms latency. Furthermore, thermal management becomes critical for sustained operation, as throttling would introduce unacceptable latency variation.

\section{Contributions}\label{sec:contributions}

This paper presents the following technical contributions to the fields of real-time audio processing, browser-based computing, and accessible music technology.

We introduce a novel hybrid architecture that strategically partitions processing responsibilities between classical DSP algorithms and neural network inference based on latency criticality. The pitch detection and spectral analysis pipeline executes on a dedicated AudioWorklet thread using optimized implementations of the YIN algorithm and Cooley-Tukey Fast Fourier Transform, ensuring deterministic sub-millisecond processing time per audio frame. Neural harmonic generation through MusicRNN executes asynchronously during browser idle periods, leveraging the \texttt{requestIdleCallback} API to prevent interference with time-critical audio paths while still providing musically meaningful accompaniment.

We present an optimized AudioWorklet implementation that achieves 50--60ms audio processing latency through careful buffer management, precomputed lookup tables for trigonometric operations, and a sliding window accumulation strategy that balances temporal resolution against frequency resolution requirements. This implementation demonstrates that professional-grade latency, previously achievable only through native code and specialized audio interfaces, can now be attained in standard web browsers executing JavaScript code.

We provide comprehensive evaluation through 235 automated tests that verify functional correctness across the full system stack, from low-level pitch detection algorithms through high-level user interaction flows. This testing infrastructure, combined with detailed latency profiling and complexity analysis, establishes confidence in system reliability and provides a foundation for continued development and optimization.

We demonstrate embedded deployment on the Raspberry Pi 5 platform, achieving 18--24ms end-to-end latency through native C++ implementation with FFTW3 ARM NEON optimization, PREEMPT\_RT real-time kernel, and TensorFlow Lite neural inference. This represents a 2.5$\times$ latency improvement over the browser implementation while maintaining identical algorithmic functionality, validating that professional-grade musical instrument performance can be achieved on an \$80 single-board computer.

To our knowledge, this work represents the first demonstration of real-time Google Magenta MusicRNN integration with browser-based pitch detection for live accompaniment generation, and the first implementation of such a system on Raspberry Pi 5 embedded hardware, establishing new capabilities for AI-augmented musical performance accessible through both web browsers and dedicated embedded instruments.

\section{Paper Organization}\label{sec:organization}

The remainder of this paper proceeds as follows. Chapter~\ref{chap:related} situates our work within the broader research landscape through comprehensive review of pitch detection algorithms, neural audio synthesis techniques, and web audio processing capabilities. Chapter~\ref{chap:system} presents our browser-based system architecture and implementation in detail, explaining design decisions and their rationale. Chapter~\ref{chap:hardware} describes the embedded hardware deployment on Raspberry Pi 5, including real-time Linux configuration, native code optimization, and thermal management. Chapter~\ref{chap:evaluation} describes our evaluation methodology and presents experimental results demonstrating system performance across both deployment configurations. Chapter~\ref{chap:discussion} discusses current limitations and outlines promising directions for future research. Chapter~\ref{chap:conclusion} concludes with a summary of contributions and their implications for accessible music technology.

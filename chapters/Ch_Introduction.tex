\chapter{Introduction}\label{chap:introduction}

\section{Background and Motivation}\label{sec:background}

The aspiration to transform the human voice into instrumental sounds has captivated musicians, engineers, and researchers for decades, representing one of the most intuitive yet technically demanding challenges in music technology. Unlike traditional musical instruments that require years of physical training to master, the human voice represents a universally accessible input modality that every individual can control with remarkable precision and expressiveness from birth. This fundamental observation motivates our investigation into voice-controlled synthesis systems that could potentially lower the barriers to musical expression and enable individuals without formal instrumental training to participate in music creation.

Beyond creative expression, recent neuroscience and clinical research has established music's profound effects on human health and wellbeing. Music engages the brain's mesolimbic dopamine reward system---the same neural circuitry processing fundamental rewards such as food and social bonding~\cite{ferreri2019dopamine}. Vocal music production triggers neurochemical cascades including increased serotonin, dopamine, and oxytocin while reducing cortisol~\cite{alaska2025singing}. These biological mechanisms translate into measurable clinical outcomes: systematic reviews demonstrate that music therapy significantly reduces anxiety and depression while improving subjective wellbeing~\cite{frontiers2025musicwellbeing}. From an evolutionary perspective, music likely preceded syntactically guided language, serving adaptive functions in social cohesion and emotional regulation~\cite{savage2014evolution}. This deep integration into human neurobiology suggests that accessible musical interaction tools could serve not merely recreational purposes but function as scalable interventions addressing mental health and neurological rehabilitation needs.

The landscape of AI-powered music technology has undergone revolutionary transformation in recent years, fundamentally reshaping expectations for what machines can accomplish in creative audio domains. The emergence of text-to-music generation platforms, most notably Suno AI and Udio, has demonstrated that end-to-end music creation from natural language descriptions has become commercially viable. Suno, valued at \$500 million following its May 2024 Series B funding, employs a hybrid architecture combining transformer-based language models with diffusion components to generate production-quality music from text prompts, while Udio, developed by former Google DeepMind researchers, applies spectrogram-domain diffusion techniques inspired by Stable Diffusion to achieve remarkable audio fidelity. These systems represent the culmination of research trajectories initiated by Google's MusicLM in January 2023~\cite{agostinelli2023musiclm}, which first demonstrated high-fidelity text-conditioned music generation through hierarchical sequence-to-sequence modeling at 24kHz sample rate. The subsequent release of Meta's open-source MusicGen in June 2023~\cite{copet2023musicgen} further accelerated progress by enabling community-driven development and customization. However, these generative systems operate fundamentally differently from the real-time interactive paradigm we address: they require seconds to minutes of computation to produce audio, precluding the immediate feedback loop essential for expressive musical performance.

The distinction between generative music AI and real-time musical interaction represents a critical axis along which our work is positioned. While Suno and Udio generate complete musical works from textual descriptions, and Stability AI's Stable Audio 2.0 produces coherent three-minute tracks through latent diffusion with timing conditioning~\cite{evans2024stablediffusion}, these systems cannot respond to performer input with the sub-100ms latency required for the sensation of instrumental control. Recent research on ``live music models'' has begun addressing this gap, with systems such as Magenta RealTime demonstrating continuous music generation with synchronized user control. However, these approaches typically require substantial GPU resources and exhibit latencies of several seconds between input and output, making them unsuitable for the tight feedback loop that characterizes traditional instrumental performance. Our work targets this underexplored intersection: real-time, low-latency vocal-to-instrument transformation with AI accompaniment, deployed accessibly through standard web browsers without specialized hardware requirements.

Traditional approaches to vocal-instrumental transformation have historically required substantial investments in specialized hardware, including dedicated MIDI wind controllers, electronic wind instruments such as the Yamaha WX series, or sophisticated pitch-to-MIDI converters that demand careful acoustic isolation and calibration. These solutions, while effective within their operational parameters, impose significant financial and technical barriers that limit accessibility to professional musicians and well-funded educational institutions. Moreover, the latency characteristics of many existing systems exceed the approximately 100ms threshold identified by Wessel and Wright~\cite{wessel2002problems} as the upper bound for perceived simultaneity in musical performance, fundamentally compromising the intimate connection between performer gesture and sonic result that characterizes expressive instrumental playing. The MIT Media Lab's recent work on AI-augmented musical instruments~\cite{sarkar2024aiaugmented} emphasizes that generative AI systems embedded within instruments must provide extensive control and responsiveness to real-time musical inputs---a design philosophy that directly informs our architectural decisions.

The emergence of neural audio synthesis techniques over the past five years has dramatically expanded the possibilities for audio transformation and generation. Pioneering work on Differentiable Digital Signal Processing (DDSP) by Engel et al.~\cite{engel2020ddsp} demonstrated that combining traditional signal processing primitives with learned neural components could achieve remarkable timbral quality while maintaining interpretability, with the DDSP-VST plugin enabling real-time timbre transfer in standard digital audio workstations. Subsequently, the Realtime Audio Variational autoEncoder (RAVE) introduced by Caillon and Esling~\cite{caillon2021rave} achieved 25x real-time synthesis at 48kHz sample rate on CPU, suggesting that neural approaches could eventually meet the stringent latency requirements of live performance. The recent BRAVE variant~\cite{caspe2025brave} achieves $\pm$3ms latency with only 4.9 million parameters while preserving timbre-transfer quality. However, deploying these sophisticated models within browser environments presents additional constraints: model weights must be transmitted over networks, inference must execute within JavaScript or WebAssembly runtime environments, and computational resources cannot be assumed to include GPU acceleration. These constraints motivate our hybrid architecture that strategically allocates neural computation to non-latency-critical tasks while relying on optimized classical algorithms for the time-sensitive pitch detection pipeline.

Concurrent with advances in neural audio synthesis, web browser capabilities have evolved substantially through standardization efforts led by the World Wide Web Consortium (W3C). The introduction of the AudioWorklet API in 2018 addressed fundamental architectural limitations of earlier browser audio processing by enabling computation on dedicated real-time threads, isolated from the garbage collection pauses and rendering overhead that characterize main-thread JavaScript execution. Performance analysis conducted by Mozilla Corporation~\cite{mozilla2020audioworklet} demonstrates that modern AudioWorklet implementations can process buffers as small as 128 samples---approximately 3 milliseconds at standard 44.1kHz sample rate---representing a dramatic improvement over the minimum 2048-sample buffers required by the deprecated ScriptProcessorNode interface. Chrome's ``Output Buffer Bypass'' feature further reduces latency by eliminating buffering stages from the audio output path. These developments, combined with emerging WebGPU capabilities for GPU-accelerated computation in browsers, suggest that the browser platform has matured sufficiently to support professional-grade audio applications that previously required native code execution.

The democratization potential of browser-based music technology extends beyond mere convenience. While Suno reports that GPU compute exceeds payroll costs by several multiples, and professional AI music performance systems require custom-built gaming PCs with high-performance GPUs, browser-based approaches can reach any user with a standard laptop and microphone. This accessibility matters particularly for healthcare applications: a 2025 study deployed an IoT-based music therapy system for elderly anxiety management in Karachi, serving 500 participants through automated physiological monitoring and therapeutic music delivery~\cite{iotmt2025elderly}. However, such systems still require specialized hardware infrastructure. Browser-based approaches could extend similar therapeutic benefits to populations with only commodity devices and internet connectivity. Client-side processing additionally addresses privacy concerns critical in healthcare contexts by ensuring that sensitive vocal data never leaves the user's device---a guarantee that cloud-based services cannot provide. This proves especially important given recent controversies regarding audio data usage, including the June 2024 Recording Industry Association of America lawsuit against Suno and Udio regarding training data practices.

\section{Problem Statement}\label{sec:problem}

This work investigates whether real-time, expressive vocal-to-instrument synthesis with sub-100ms latency can be achieved using only browser-based technologies while simultaneously providing AI-powered harmonic accompaniment that enhances the musical experience. The sub-100ms latency target reflects not merely a technical benchmark but a threshold identified in music therapy research as necessary for maintaining the sense of control and immediate feedback essential to therapeutic engagement~\cite{wessel2002problems}. This research question encompasses several interrelated technical challenges that must be addressed systematically.

The Web Audio API, while providing powerful primitives for audio processing, introduces multiple buffering stages that accumulate latency throughout the signal path from microphone input to speaker output. Each buffer represents a fundamental tradeoff between latency and computational headroom, as smaller buffers provide lower latency but leave less time for completing processing before the next buffer arrives. Our investigation must therefore identify buffer configurations and processing architectures that minimize cumulative latency while maintaining sufficient headroom for computationally intensive analysis algorithms.

JavaScript execution characteristics present additional challenges for real-time audio applications. The language's garbage collection mechanism can introduce unpredictable pauses that, if occurring during audio processing, manifest as audible discontinuities that severely degrade user experience. Furthermore, JavaScript's single-threaded execution model historically meant that computationally intensive operations would block UI responsiveness, creating an inherent tension between processing depth and interface fluidity. Our architecture must leverage modern browser threading capabilities to isolate time-critical audio processing from non-deterministic main-thread activities.

Algorithm selection for pitch detection presents a nuanced optimization problem that balances accuracy, computational cost, and latency characteristics. Neural pitch detectors such as CREPE~\cite{kim2018crepe} achieve superior accuracy in noisy conditions but require model inference that may introduce latency incompatible with real-time requirements. Classical algorithms including autocorrelation and the YIN algorithm~\cite{decheveigne2002yin} provide deterministic execution time but may sacrifice accuracy under adverse acoustic conditions. Our system must select and optimize algorithms appropriate for the target application domain of clean vocal input from consumer microphones.

Finally, integrating neural network inference for harmonic accompaniment generation requires careful architectural consideration to prevent machine learning computation from blocking time-critical audio processing. Neural sequence generation through models such as MusicRNN requires substantial computation that cannot complete within typical audio buffer periods, necessitating asynchronous execution strategies that decouple inference timing from audio synthesis.

\section{Contributions}\label{sec:contributions}

This paper presents the following technical contributions to the fields of real-time audio processing, browser-based computing, and accessible music technology.

We introduce a novel hybrid architecture that strategically partitions processing responsibilities between classical DSP algorithms and neural network inference based on latency criticality. The pitch detection and spectral analysis pipeline executes on a dedicated AudioWorklet thread using optimized implementations of the YIN algorithm and Cooley-Tukey Fast Fourier Transform, ensuring deterministic sub-millisecond processing time per audio frame. Neural harmonic generation through MusicRNN executes asynchronously during browser idle periods, leveraging the \texttt{requestIdleCallback} API to prevent interference with time-critical audio paths while still providing musically meaningful accompaniment.

We present an optimized AudioWorklet implementation that achieves 50--60ms audio processing latency through careful buffer management, precomputed lookup tables for trigonometric operations, and a sliding window accumulation strategy that balances temporal resolution against frequency resolution requirements. This implementation demonstrates that professional-grade latency, previously achievable only through native code and specialized audio interfaces, can now be attained in standard web browsers executing JavaScript code.

We provide comprehensive evaluation through 235 automated tests that verify functional correctness across the full system stack, from low-level pitch detection algorithms through high-level user interaction flows. This testing infrastructure, combined with detailed latency profiling and complexity analysis, establishes confidence in system reliability and provides a foundation for continued development and optimization.

To our knowledge, this work represents the first demonstration of real-time Google Magenta MusicRNN integration with browser-based pitch detection for live accompaniment generation, establishing a new capability for AI-augmented musical performance accessible to anyone with a web browser.

\section{Paper Organization}\label{sec:organization}

The remainder of this paper proceeds as follows. Chapter~\ref{chap:related} situates our work within the broader research landscape through comprehensive review of pitch detection algorithms, neural audio synthesis techniques, and web audio processing capabilities. Chapter~\ref{chap:system} presents our software system architecture and implementation in detail, explaining design decisions and their rationale. Chapter~\ref{chap:hardware} describes the custom hardware audio acquisition system that provides an alternative, low-latency input path. Chapter~\ref{chap:evaluation} describes our evaluation methodology and presents experimental results demonstrating system performance across multiple dimensions. Chapter~\ref{chap:discussion} discusses current limitations and outlines promising directions for future research. Chapter~\ref{chap:conclusion} concludes with a summary of contributions and their implications for accessible music technology.

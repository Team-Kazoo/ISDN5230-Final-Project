\chapter{System Design and Implementation}\label{chap:system}

\section{Architecture Overview}\label{sec:architecture}

The Mambo Whistle system employs a five-layer architecture that enforces strict separation of concerns across functionally distinct subsystems while enabling efficient inter-layer communication through well-defined interfaces. This architectural organization reflects careful analysis of the diverse requirements spanning real-time audio processing, neural network inference, state management, and user interface rendering, with each layer optimized for its specific computational characteristics and latency constraints.

The foundational Neural Synthesis Layer encapsulates integration with Google Magenta's MusicRNN for AI-powered harmonic accompaniment generation. This layer operates asynchronously with respect to the audio processing pipeline, receiving melodic context through accumulated pitch detection results and generating accompaniment sequences during browser idle periods. The asynchronous execution model prevents neural network inference, which requires 250--500 milliseconds depending on hardware capabilities, from interfering with the strict timing requirements of audio processing.

The Audio Processing Layer executes on a dedicated AudioWorklet thread, implementing the latency-critical pitch detection, spectral analysis, and signal conditioning algorithms that transform raw microphone input into synthesis control parameters. This layer maintains complete isolation from main-thread activities including garbage collection and UI rendering, ensuring deterministic processing timing essential for consistent audio quality. Communication with other layers occurs exclusively through MessagePort interfaces that impose minimal synchronization overhead.

The Business Logic Layer contains manager components that encapsulate domain-specific functionality and orchestrate interactions between the audio processing pipeline, neural synthesis, and user interface. The AudioLoopController coordinates the flow of pitch detection results to synthesis engines and visualization components while maintaining performance metrics. The SynthManager implements the Strategy pattern to enable runtime switching between continuous and discrete synthesis modes without architectural modification. The DeviceManager handles audio input/output device enumeration, selection persistence, and permission management required by browser security models.

The State Management Layer provides centralized application state storage through a Flux-inspired publish-subscribe architecture. This design ensures unidirectional data flow that simplifies reasoning about state changes while enabling efficient notification of interested components. The StateStore maintains four semantically distinct state slices encompassing engine status, audio configuration, synthesis parameters, and user interface state, with immutable update semantics that prevent accidental state corruption.

The User Interface Layer implements state-driven rendering through the MamboView component, which translates StateStore changes into DOM updates without maintaining independent state that could diverge from the authoritative store. This architecture eliminates the class of bugs arising from state synchronization failures while enabling efficient incremental rendering that minimizes layout recalculation overhead. The visualization subsystem operates independently at 60 frames per second, providing smooth real-time feedback even when other UI components update less frequently.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/fig3_architecture.jpg}
  \caption{System architecture diagram illustrating the five-layer organization with explicit indication of threading boundaries. The diagram employs distinct visual treatment for the AudioWorklet thread (containing the Audio Processing Layer) versus the main thread (containing remaining layers), with MessagePort interfaces clearly depicted as the communication mechanism crossing this boundary. Color coding differentiates layers while connection styling indicates synchronous versus asynchronous communication patterns.}
  \label{fig:system_architecture}
\end{figure}

\section{Audio Processing Pipeline}\label{sec:pipeline}

The audio processing pipeline represents the performance-critical core of the Mambo Whistle system, transforming raw microphone input into the frequency, amplitude, and timbral parameters that drive synthesis and visualization. The pipeline architecture reflects systematic optimization for minimal latency while maintaining the analysis quality required for accurate pitch detection and expressive synthesis control.

Audio capture initiates through the \texttt{MediaDevices.getUserMedia} API, which provides access to microphone input as a MediaStream. The requested audio constraints explicitly disable browser-provided processing including echo cancellation, noise suppression, and automatic gain control, as these algorithms introduce both latency and signal modifications that degrade pitch detection accuracy. The raw audio stream connects to a MediaStreamSource node that serves as the entry point into the Web Audio API processing graph.

The AudioWorkletProcessor receives audio in fixed-size frames of 128 samples, representing approximately 2.9 milliseconds at 44.1kHz sample rate. However, accurate pitch detection for frequencies as low as 80Hz---corresponding to the lower range of human voice fundamental frequencies---requires analysis windows spanning at least several complete periods of the lowest target frequency. Our implementation addresses this resolution requirement through a sliding window accumulation strategy that buffers incoming samples until a 1024-sample analysis window is complete, then advances by half the window length to maintain 50\% overlap between successive analysis frames.

The YIN algorithm implementation processes each complete analysis window to estimate fundamental frequency with sub-semitone accuracy. Our implementation follows the four-stage structure defined by de Cheveign√© and Kawahara~\cite{decheveigne2002yin}, beginning with squared difference function computation that measures signal self-similarity across all lag values up to half the window length. The cumulative mean normalized difference function then adjusts these similarity measures to eliminate systematic bias toward longer periods, effectively normalizing for the expected variance at each lag. Threshold-based period selection identifies the shortest lag where the normalized difference falls below 0.15, with this threshold value optimized through empirical testing on vocal input representative of our target application. Finally, parabolic interpolation refines the period estimate to sub-sample precision, improving frequency resolution beyond the limit imposed by discrete sampling.

Spectral analysis proceeds through a Fast Fourier Transform implementation employing the Cooley-Tukey radix-2 algorithm. Our implementation achieves $O(N \log N)$ complexity through iterative butterfly operations organized in $\log_2(N)$ stages, with each stage computing $N/2$ complex multiplications. Precomputed twiddle factor tables eliminate trigonometric function evaluation during transform computation, as these tables require only 22.5 kilobytes of memory and can be computed once during initialization. The resulting power spectrum enables extraction of perceptually relevant features including spectral centroid, which correlates with perceived brightness, and spectral flatness, which indicates the noise-like versus tonal character of the signal.

Signal conditioning through Kalman and exponential moving average filters reduces frame-to-frame jitter in extracted parameters that would otherwise produce unpleasant synthesis artifacts. The Kalman filter applied to pitch deviation (cents from nearest semitone) provides optimal recursive estimation balancing responsiveness against noise rejection, with process and measurement noise parameters tuned for the characteristic variation rates of sung input. Exponential moving average filters with empirically selected time constants smooth amplitude and timbral parameters while preserving the dynamic response essential for expressive synthesis control.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{figures/fig4_pipeline.jpg}
  \caption{Audio processing pipeline data flow diagram with latency annotations at each stage. The visualization depicts the complete signal path from microphone through analysis stages to synthesis control output, with timing measurements indicating contribution of each stage to overall latency. The threading boundary between AudioWorklet and main thread is prominently indicated, with MessagePort communication depicted as the cross-thread synchronization point.}
  \label{fig:audio_pipeline}
\end{figure}

\section{Pitch Detection Implementation}\label{sec:pitch_impl}

The YIN algorithm implementation within our AudioWorklet processor reflects careful optimization for the computational constraints and latency requirements of real-time browser execution. The algorithm transforms a 1024-sample time-domain buffer into a fundamental frequency estimate through a sequence of operations that we detail with attention to complexity characteristics and implementation decisions.

The squared difference function computation constitutes the dominant computational cost, requiring evaluation across all lag values $\tau$ from 1 to $N/2$ where $N$ represents the analysis window length. For each lag value, the function computes the sum of squared differences between the original signal and a version shifted by $\tau$ samples, requiring $N/2$ subtraction and multiplication operations per lag value. The complete computation therefore requires $(N/2)^2 = 262,144$ multiply-accumulate operations for our 1024-sample window, establishing an $O(N^2)$ complexity that dominates the per-frame computational budget. However, this computation completes in approximately 500 microseconds on modern hardware, leaving substantial headroom within the 23-millisecond frame period defined by our analysis window advance rate.

Cumulative mean normalization transforms the squared difference values to eliminate the systematic bias toward longer periods inherent in the unnormalized function. This transformation computes, for each lag $\tau$, the ratio of the squared difference value to the running average of all preceding values, producing the cumulative mean normalized difference function $d'(\tau)$. The normalization ensures that the function value at the true fundamental period will appear as a local minimum relative to surrounding values regardless of the absolute frequency, simplifying subsequent period selection. The computational cost of $O(N)$ for this stage proves negligible relative to the squared difference computation.

Period selection proceeds by scanning the normalized difference function to identify the smallest lag where $d'(\tau)$ falls below the threshold $\theta = 0.15$. Upon finding such a lag, the algorithm continues scanning to locate the subsequent local minimum, ensuring selection of the lag corresponding to minimum normalized difference rather than merely the first sub-threshold value. This refinement proves essential for robustness against spurious minima that can occur at sub-period lags due to harmonic structure. If no sub-threshold lag is found, indicating insufficient signal periodicity for confident pitch detection, the algorithm returns null to indicate detection failure rather than producing an unreliable estimate.

Parabolic interpolation refines the discrete lag estimate to sub-sample precision by fitting a quadratic function through the selected minimum and its immediate neighbors. The refined lag $\tau'$ minimizes the interpolated parabola, achieving frequency resolution beyond the limit imposed by discrete sampling. For a sample rate of 44.1kHz and analysis window of 1024 samples, unrefined lag estimation provides frequency resolution of approximately 86Hz at a 500Hz fundamental---clearly inadequate for musical applications where semitones span only 6\% frequency ratios. Parabolic interpolation improves resolution to better than 1Hz across the vocal frequency range, enabling accurate pitch cents calculation for synthesis control and visualization.

The confidence assessment combines multiple indicators to estimate the reliability of each pitch detection result. Primary confidence derives from the normalized difference value at the detected period, with lower values indicating stronger periodicity and more reliable estimates. Secondary confidence factors include signal amplitude, with very quiet signals producing unreliable estimates regardless of apparent periodicity, and frequency range validation, with estimates outside the 80--800Hz vocal range flagged as potentially erroneous. The composite confidence value enables downstream processing to weight or reject low-confidence estimates appropriately.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/fig5_yin_algorithm.png}
  \caption{Visualization of YIN algorithm processing stages applied to a representative vocal input segment. The figure presents four vertically arranged panels showing: (a) the input waveform with visible periodic structure, (b) the squared difference function with characteristic minimum at the fundamental period, (c) the cumulative mean normalized difference with threshold line indicating detection criterion, and (d) the refined period estimate with parabolic interpolation illustrated.}
  \label{fig:yin_visualization}
\end{figure}

\section{Spectral Feature Extraction}\label{sec:spectral}

Beyond fundamental frequency estimation, our system extracts spectral features that enable synthesis control responsive to the timbral characteristics of vocal input. These features quantify perceptually salient aspects of the frequency-domain signal representation, providing continuous control dimensions that enhance the expressiveness achievable through voice-controlled synthesis.

The Fast Fourier Transform implementation employs the Cooley-Tukey decimation-in-time algorithm, computing the discrete Fourier transform of the 1024-sample analysis window with $O(N \log N)$ complexity. This represents a substantial improvement over the $O(N^2)$ naive DFT computation, reducing the required operations from approximately one million to under six thousand for our analysis window size. The implementation achieves this efficiency through recursive decomposition of the $N$-point transform into successively smaller transforms, ultimately reducing to trivial 2-point transforms computed directly. Butterfly operations recombine results at each stage, with twiddle factor multiplications applying the required phase rotations.

Our implementation further optimizes FFT computation through precomputed lookup tables for sine and cosine values required by twiddle factors. These trigonometric functions would otherwise require expensive evaluation during each transform, as their arguments depend on both the stage index and the position within each stage. By precomputing the complete set of required values during initialization and storing them in \texttt{Float32Array} structures, we eliminate this overhead at the cost of 8.2 kilobytes of memory---an entirely acceptable tradeoff given modern device capabilities. Additionally, we precompute the bit-reversal permutation table required for input reordering, further reducing per-transform computation.

The spectral centroid feature provides a measure of brightness, computed as the amplitude-weighted average of frequency components across the analysis bandwidth. Mathematically, the centroid equals the sum of each frequency bin multiplied by its magnitude, divided by the sum of magnitudes. This computation has direct perceptual correlates: sounds with energy concentrated at higher frequencies exhibit higher centroid values and are perceived as brighter, while sounds with low-frequency dominance produce lower centroid values perceived as darker or more mellow. Our implementation normalizes the centroid relative to the Nyquist frequency to produce a 0--1 range suitable for direct use as a synthesis control parameter.

Spectral flatness, also known as Wiener entropy, quantifies the noise-like versus tonal character of the signal through the ratio of geometric to arithmetic mean of spectral magnitudes. A pure tone produces zero spectral flatness as its geometric mean approaches zero when computed over the full spectrum, while white noise with uniform spectral distribution produces flatness approaching unity. Human voice exhibits intermediate flatness values that vary with phonetic content---sustained vowels produce low flatness while fricative consonants and breathy phonation produce higher values. We map this feature to a ``breathiness'' control parameter that modulates noise injection in the synthesis engine, enabling timbral variation that tracks the acoustic character of vocal input.

The frequency resolution achieved by our 1024-point FFT at 44.1kHz sample rate equals approximately 43Hz per bin, providing adequate resolution for spectral feature computation though insufficient for direct pitch estimation of lower frequencies. This observation reinforces our architectural decision to employ time-domain YIN analysis for pitch detection while reserving FFT analysis for spectral feature extraction where coarser resolution suffices.

\section{Neural Harmonic Generation}\label{sec:neural_harmonic}

The AI Harmonizer module extends the real-time synthesis capabilities of Mambo Whistle through integration of Google Magenta's MusicRNN, enabling automatic generation of harmonic accompaniment that responds to the performer's melodic input. This integration demonstrates the feasibility of AI-augmented musical interaction within browser execution environments while respecting the strict latency constraints imposed by real-time audio processing.

The MusicRNN model operates on symbolic music representations rather than audio waveforms, accepting sequences of MIDI note events as input and generating probabilistic continuations that reflect patterns learned during training on large musical corpora. Our integration employs the \texttt{melody\_rnn} checkpoint optimized for single-voice melodic generation, which provides appropriate accompaniment for the monophonic vocal input our system processes. The model parameters loaded from Google's cloud storage require approximately 5 megabytes, with initialization completing in 2--3 seconds on typical hardware---a one-time cost amortized across the duration of each session.

The integration architecture maintains a circular buffer of pitch detection results converted to MIDI note numbers through the standard equal-tempered frequency-to-MIDI mapping. This buffer accumulates melodic context during performance, filtering for confidence values exceeding 0.7 to reject spurious detections while employing deduplication logic to avoid recording repeated instances of sustained pitches. When the buffer contains sufficient context---at least 5 distinct notes---and adequate time has elapsed since the previous generation---2 seconds in our current configuration---the system initiates accompaniment generation.

The scheduling strategy for neural network inference exploits the \texttt{requestIdleCallback} browser API to execute computationally intensive model evaluation during periods when the browser would otherwise be idle. This approach prevents the 250--500 milliseconds required for sequence generation from blocking time-critical audio processing or degrading UI responsiveness. The \texttt{requestIdleCallback} API provides a deadline parameter indicating available idle time, enabling workloads to yield gracefully when the browser requires thread availability for higher-priority tasks. We specify a 1-second timeout to ensure generation eventually proceeds even under sustained browser activity, accepting the potential for brief responsiveness reduction in exchange for reliable accompaniment generation.

The generated note sequence feeds to a Tone.js PolySynth instance configured with lush pad characteristics appropriate for background accompaniment. The synthesizer employs detuned sawtooth oscillators that provide harmonic richness while avoiding the harshness of pure sawtooth tones, with envelope parameters optimized for smooth attack and extended release that allows generated notes to decay naturally without abrupt termination. Reverb effects provide spatial depth that places the accompaniment behind the performer's primary synthesis, establishing appropriate foreground-background relationships in the resulting audio.

The temperature parameter controlling sequence generation stochasticity is set to 1.1, slightly above the neutral value of 1.0. This configuration introduces sufficient randomness to prevent repetitive or predictable accompaniment while maintaining musical coherence with the input melody. Higher temperature values would produce more surprising but potentially less stylistically appropriate generations, while lower values would yield safer but potentially monotonous output.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/fig6_ai_harmonizer.jpg}
  \caption{AI Harmonizer data flow and timing diagram illustrating the asynchronous relationship between pitch detection, note buffering, and neural generation. The visualization depicts the temporal structure showing continuous pitch detection feeding the note buffer, periodic generation triggers, and the decoupled timing of accompaniment playback relative to user input. Timing annotations indicate typical latencies at each stage, demonstrating that generation latency does not impact real-time pitch detection and synthesis.}
  \label{fig:ai_harmonizer}
\end{figure}

\section{Synthesis Engine Architecture}\label{sec:synthesis}

The synthesis subsystem provides two distinct operational modes optimized for different musical applications and performer preferences, implementing the Strategy pattern to enable runtime mode switching without architectural modification. This dual-engine architecture reflects recognition that different musical contexts benefit from fundamentally different mappings between detected pitch and synthesized output.

Continuous Mode implements direct frequency tracking that preserves the continuous pitch variations characteristic of natural vocal production. The synthesis oscillator frequency updates smoothly to match detected fundamental frequency, with configurable portamento time controlling the rate of frequency transitions. This mode faithfully reproduces vibrato, pitch slides, and the subtle intonation variations that distinguish expressive singing from mechanical pitch sequences. The mode proves particularly suitable for emulating instruments such as violin, cello, or voice itself, where continuous pitch variation constitutes an essential expressive dimension.

Legacy Mode implements pitch quantization that maps detected frequencies to the nearest semitone of the equal-tempered scale before driving synthesis. This processing produces discrete note transitions characteristic of keyboard instruments, where pitch remains stable until the performer initiates a distinct new note. The quantization boundary at $\pm$50 cents from each semitone determines when transitions occur, with hysteresis preventing rapid switching when the input frequency hovers near a boundary. This mode suits emulation of piano, organ, and other fixed-pitch instruments while simplifying the performer's task by eliminating the need for precise intonation.

Both modes share common synthesis architecture built on Tone.js, providing access to a rich palette of oscillator types, filter configurations, and effect processors. The instrument preset system packages oscillator configurations, envelope parameters, filter settings, and effect routing into named presets corresponding to familiar instrument categories. Presets for flute, saxophone, violin, cello, and various synthesizer voices provide immediate sonic variety while serving as starting points for user customization through exposed parameter controls.

Expressive control parameters beyond pitch include amplitude, brightness, and breathiness, each derived from analysis results and mapped to synthesis parameters through configurable response curves. Amplitude controls overall output level and can additionally modulate filter cutoff and oscillator brightness for dynamics-responsive timbral variation. The brightness parameter derived from spectral centroid modulates filter cutoff frequency, producing brighter synthesis tones when the vocal input exhibits high-frequency emphasis. Breathiness modulates injection of filtered noise into the synthesis output, adding air and texture that tracks the aspiration content of vocal input.

\section{State Management and User Interface}\label{sec:state_ui}

Application state management through the centralized StateStore ensures predictable, traceable state evolution while enabling efficient UI updates through publish-subscribe notification. The store maintains four semantically distinct state slices that collectively represent the complete application state required for rendering and behavior.

The status slice tracks engine lifecycle state and error conditions, distinguishing idle, starting, running, and error states that determine UI presentation and available user actions. The audio slice maintains device selection, enumeration results, and latency measurements that enable informed user configuration and performance feedback. The synth slice stores synthesis mode selection, instrument preset, effect parameters, and auto-tune configuration. The ui slice tracks presentation state including modal visibility and theme selection that affect rendering without influencing audio behavior.

State updates proceed through the \texttt{setState} method, which accepts partial state specifications and performs shallow merging with existing state. The implementation explicitly copies state objects to prevent inadvertent mutation, as mutation would violate the unidirectional data flow principle and potentially cause subtle bugs where components observe unexpected state changes. Following state update, the store notifies all registered subscribers, which typically include UI components that should re-render to reflect the new state.

The MamboView component implements state-driven rendering by translating StateStore contents into DOM structure and attributes. Rather than maintaining internal state that could diverge from the authoritative store, MamboView queries current state during each render cycle and applies necessary DOM updates. The implementation optimizes rendering by comparing new values against current DOM state and skipping updates when values match, avoiding unnecessary layout recalculation that would degrade UI performance. This conditional update strategy proves particularly important for high-frequency state changes such as pitch visualization, where naive rendering would overwhelm browser layout capabilities.

Event binding follows the delegation pattern, with MamboView establishing event listeners during initialization and delegating received events to handler functions provided by the business logic layer. This separation ensures that view components contain no business logic, maintaining clear boundaries between presentation and behavior while facilitating testing through handler substitution.

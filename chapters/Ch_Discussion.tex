\chapter{Discussion and Future Work}\label{chap:discussion}

\section{Limitations and Constraints}\label{sec:limitations}

Despite the substantial capabilities demonstrated by Mambo Whistle across both browser and embedded deployments, several limitations constrain current system applicability and suggest directions for continued development. Forthright acknowledgment of these limitations contextualizes our contributions and identifies opportunities for impactful future research.

The achieved browser latency of 50--60 milliseconds, while representing a significant advancement for web-based audio processing, remains perceptible to trained musicians performing rapid passages. Research on musical performance timing indicates that performers can perceive delays as small as 20--30 milliseconds when they conflict with established sensorimotor expectations. Our embedded Raspberry Pi 5 deployment substantially addresses this limitation, achieving 18--24ms latency that approaches the perceptual threshold, but some performers may still notice delay during extremely rapid articulations. The theoretical minimum latency achievable through our current architecture approaches 10--12 milliseconds on embedded hardware, limited primarily by USB audio protocol overhead and buffer accumulation requirements for accurate low-frequency pitch detection.

The monophonic limitation inherent in YIN-based pitch detection restricts system applicability to single-voice input, precluding processing of polyphonic vocal techniques such as overtone singing or multiple simultaneous performers. While polyphonic pitch detection algorithms exist, they impose substantially higher computational costs and reduced accuracy compared to monophonic approaches, presenting challenging tradeoffs for real-time execution on both browser and embedded platforms. The current monophonic focus reflects a pragmatic scope limitation appropriate for the target application of voice-controlled instrumental synthesis.

Neural model deployment introduces platform-specific constraints. In browser environments, first-use latency as model weights download from cloud storage typically requires 2--3 seconds on broadband connections. On Raspberry Pi 5, the TensorFlow Lite runtime and quantized model weights consume approximately 180MB of the 8GB available memory, leaving adequate headroom for current functionality but potentially constraining deployment of more sophisticated neural synthesis models. The INT8 quantization applied for embedded deployment introduces minor quality degradation in generated accompaniment compared to full-precision inference, though this remains acceptable for the background accompaniment application.

The Raspberry Pi 5 embedded deployment introduces hardware-specific constraints absent from browser environments. Thermal management requires active cooling for sustained operation, with the blower fan producing audible noise (35--40dB) that may be objectionable in quiet recording environments. Power consumption of 6--8W necessitates either continuous AC power or periodic battery replacement for extended mobile use. The USB audio interface requirement adds cost and bulk compared to integrated audio solutions available on some competing single-board computers.

Browser implementation variability introduces uncertainty regarding performance characteristics across deployment targets. Despite substantial standardization progress, browser vendors retain implementation flexibility that produces observable differences in latency, computational efficiency, and API behavior. Safari's incomplete AudioWorklet support and missing \texttt{requestIdleCallback} API require fallback code paths that may degrade performance on Apple devices.

\section{Future Research Directions}\label{sec:future}

The foundation established by Mambo Whistle enables several promising research directions that could substantially extend system capabilities and broaden applicability. We outline these directions with attention to technical feasibility and potential impact.

Integration of neural pitch detection through lightweight models such as quantized CREPE variants could improve robustness in acoustically challenging environments including background noise and room reverberation. Recent work on model compression demonstrates that pitch detection networks can be quantized to achieve approximately 2 milliseconds inference time on Raspberry Pi 5, potentially compatible with real-time requirements. The architectural challenge lies in managing the inference latency contribution without substantially increasing end-to-end latency, potentially through speculative execution or confidence-based fallback to classical detection.

Migration to audio-domain neural synthesis through implementations of RAVE or DDSP would dramatically improve timbral quality and enable capabilities including timbre transfer and voice conversion. The recent BRAVE variant achieves 3ms latency with 4.9 million parameters, suggesting feasibility of deployment on Raspberry Pi 5 through TensorFlow Lite. The primary challenges involve managing model size for reasonable memory consumption and optimizing inference scheduling to maintain real-time audio generation without dropouts. Preliminary experiments suggest that RAVE inference could achieve real-time performance on Raspberry Pi 5 with appropriate quantization, though this remains an active area of investigation.

FPGA acceleration for the pitch detection pipeline could reduce latency below 5 milliseconds by implementing the YIN algorithm in dedicated hardware. The Raspberry Pi 5's PCIe interface could accommodate FPGA accelerator cards, enabling sub-millisecond pitch detection while freeing CPU resources for more sophisticated neural synthesis. This approach would substantially increase system cost and complexity but could enable latency performance competitive with dedicated electronic musical instruments.

Alternative embedded platforms merit investigation for specific deployment scenarios. The NVIDIA Jetson Nano and Orin series provide GPU acceleration that could enable deployment of audio-domain neural synthesis models currently impractical on CPU-only platforms. The BeagleBone AI-64 integrates dedicated DSP and machine learning accelerators that could provide superior performance-per-watt for battery-operated applications. Comparative evaluation across these platforms would inform platform selection for different use cases.

Custom audio hardware integration could eliminate USB protocol overhead that contributes 8--10ms to current embedded latency. Direct I2S interface to the Raspberry Pi 5 GPIO would enable DMA-based audio transfer with sub-millisecond latency, though this would require custom PCB development for microphone preamplification and phantom power supply. The HiFiBerry ADC+DAC Pro and similar I2S audio HATs provide a middle ground, offering improved latency over USB while maintaining commercial availability.

Collaborative musical interaction through WebRTC integration could enable real-time ensemble performance over internet connections, with latency compensation algorithms maintaining musical synchronization despite network delays. The embedded Raspberry Pi 5 deployment could serve as a dedicated network endpoint with lower and more consistent latency than browser-based participants, potentially serving as a ``hub'' device for distributed musical collaboration.

\section{Broader Impact}\label{sec:impact}

The demonstrated feasibility of professional-grade audio synthesis across both browser and embedded platforms carries implications extending beyond the immediate application domain of voice-controlled instruments. These broader impacts merit consideration as they inform assessment of contribution significance and suggest additional application directions.

Music education stands to benefit substantially from both deployment modalities. Browser-based access eliminates IT support overhead for classroom deployment, enabling teachers to direct students to web URLs providing immediate access to expressive instruments without software installation. The embedded Raspberry Pi 5 configuration enables construction of dedicated classroom instruments at approximately \$150 per unit (including enclosure and audio interface), substantially lower than commercial electronic instruments with comparable capabilities.

Accessibility for musicians with motor impairments represents an important application direction, as voice-based control provides an alternative input modality for individuals unable to operate traditional instruments requiring manual dexterity. The embedded deployment enables construction of dedicated accessible instruments that can be customized for specific user needs, including alternative input methods such as breath sensors or eye tracking that could augment or replace voice input.

Privacy preservation through client-side processing addresses growing societal concern regarding audio surveillance and data collection. Both browser and embedded deployments ensure that sensitive vocal data never leaves the user's device, with the embedded configuration providing additional assurance through complete network isolation if desired. This guarantee proves particularly valuable for applications involving children, where data protection regulations impose stringent requirements, and for users in jurisdictions with strong privacy expectations.

The embedded deployment specifically enables applications where internet connectivity is unavailable or unreliable. Outdoor performances, remote educational settings, and developing regions with limited infrastructure can benefit from standalone operation. The 10--12 hour battery life achievable with standard USB-C power banks enables full-day operation without infrastructure requirements.

Research acceleration benefits from both deployment modalities. Browser-based deployment enables rapid prototyping and evaluation of audio algorithms with immediate access through URLs. The embedded platform enables controlled experimental conditions with deterministic timing characteristics that browser environments cannot guarantee, facilitating rigorous latency measurements and comparative evaluations against dedicated hardware.

\section{Lessons Learned}\label{sec:lessons}

The dual-track development process---simultaneously targeting browser and embedded deployment---yielded insights applicable to future projects spanning web and native platforms.

Algorithm portability between JavaScript and C++ proved straightforward when algorithms were designed with clear interfaces and minimal platform dependencies. The YIN and FFT implementations required only syntactic translation, with the C++ versions benefiting from compiler optimizations without algorithmic modification. This portability validates the architectural decision to maintain algorithmic cores in pure computational form, isolated from platform-specific I/O and threading concerns.

Performance optimization follows different strategies across platforms. Browser optimization focuses on avoiding garbage collection through object pooling and TypedArray usage, while embedded optimization emphasizes cache efficiency and SIMD utilization. These different optimization targets suggest that platform-specific profiling is essential despite shared algorithmic foundations.

The 2.5$\times$ latency improvement from browser to embedded deployment quantifies the overhead imposed by browser audio architectures. This substantial difference suggests that applications with stringent latency requirements should consider native deployment despite the development cost increase, while applications prioritizing accessibility can accept browser latency tradeoffs.

Real-time Linux configuration proved more complex than anticipated, with interactions between kernel configuration, process scheduling, and peripheral drivers requiring systematic experimentation. Documentation of successful configurations, as provided in Chapter~\ref{chap:hardware}, accelerates future deployments but cannot substitute for platform-specific tuning when hardware or software versions differ from documented configurations.

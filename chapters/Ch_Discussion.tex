\chapter{Discussion and Future Work}\label{chap:discussion}

\section{Limitations and Constraints}\label{sec:limitations}

Despite the substantial capabilities demonstrated by Mambo Whistle, several limitations constrain current system applicability and suggest directions for continued development. Forthright acknowledgment of these limitations contextualizes our contributions and identifies opportunities for impactful future research.

The achieved latency of 50--60 milliseconds, while representing a significant advancement for browser-based audio processing, remains perceptible to trained musicians performing rapid passages. Research on musical performance timing indicates that performers can perceive delays as small as 20--30 milliseconds when they conflict with established sensorimotor expectations, suggesting that further latency reduction would benefit professional applications. The theoretical minimum latency achievable through our current architecture approaches 5 milliseconds, limited by buffer accumulation requirements for accurate low-frequency pitch detection. Achieving this theoretical minimum would require optimization of buffer management strategies and potentially WebAssembly acceleration of compute-intensive algorithms.

The monophonic limitation inherent in YIN-based pitch detection restricts system applicability to single-voice input, precluding processing of polyphonic vocal techniques such as overtone singing or multiple simultaneous performers. While polyphonic pitch detection algorithms exist, they impose substantially higher computational costs and reduced accuracy compared to monophonic approaches, presenting challenging tradeoffs for real-time browser execution. The current monophonic focus reflects a pragmatic scope limitation appropriate for the target application of voice-controlled instrumental synthesis.

Neural model deployment introduces first-use latency as model weights download from cloud storage, typically requiring 2--3 seconds on broadband connections but potentially much longer on constrained networks. This initialization latency may impact user experience, particularly for users who expect immediate functionality upon page load. Progressive loading strategies or model caching through Service Workers could mitigate this limitation in future implementations.

Browser implementation variability introduces uncertainty regarding performance characteristics across deployment targets. Despite substantial standardization progress, browser vendors retain implementation flexibility that produces observable differences in latency, computational efficiency, and API behavior. Our extensive compatibility testing characterizes current browser behavior, but ongoing browser development may introduce regressions or improvements that alter system performance.

\section{Future Research Directions}\label{sec:future}

The foundation established by Mambo Whistle enables several promising research directions that could substantially extend system capabilities and broaden applicability. We outline these directions with attention to technical feasibility and potential impact.

Integration of neural pitch detection through lightweight models such as quantized CREPE variants could improve robustness in acoustically challenging environments including background noise and room reverberation. Recent work on model compression demonstrates that pitch detection networks can be quantized to achieve approximately 2 milliseconds inference time on modern CPUs, potentially compatible with real-time requirements. The architectural challenge lies in managing the inference latency contribution without substantially increasing end-to-end latency, potentially through speculative execution or confidence-based fallback to classical detection.

Migration to audio-domain neural synthesis through browser-compatible implementations of RAVE or DDSP would dramatically improve timbral quality and enable capabilities including timbre transfer and voice conversion. RAVE achieves 25x real-time performance on CPU, suggesting feasibility of browser deployment through TensorFlow.js or WebAssembly compilation. The primary challenges involve managing model size for reasonable download times and optimizing inference scheduling to maintain real-time audio generation without dropouts.

WebAssembly acceleration of compute-intensive algorithms could reduce processing time by 2--5x compared to JavaScript execution, enabling smaller buffer sizes and correspondingly lower latency. The Emscripten toolchain provides mature compilation from C++ to WebAssembly, enabling porting of optimized native implementations. The architectural challenge involves managing data transfer between JavaScript audio contexts and WebAssembly memory without introducing serialization overhead that negates computational savings.

Embedded hardware deployment through porting to platforms such as NVIDIA Jetson or Raspberry Pi 5 could achieve sub-10-millisecond latency suitable for professional musical applications while enabling standalone operation without dependency on browser environments. The modular architecture facilitating this transition would require reimplementation of the audio I/O layer for native audio APIs while preserving the algorithmic core with minimal modification.

Collaborative musical interaction through WebRTC integration could enable real-time ensemble performance over internet connections, with latency compensation algorithms maintaining musical synchronization despite network delays. This direction would extend system applicability from individual practice and performance to distributed musical collaboration, addressing the growing interest in remote music-making catalyzed by recent global circumstances.

\section{Broader Impact}\label{sec:impact}

The demonstrated feasibility of professional-grade audio synthesis within browser environments carries implications extending beyond the immediate application domain of voice-controlled instruments. These broader impacts merit consideration as they inform assessment of contribution significance and suggest additional application directions.

\subsection{Healthcare and Therapeutic Applications}

Clinical music therapy research has established robust evidence for music-based interventions across diverse health conditions. A 2025 meta-analysis examining music therapy for anxiety found significant reductions across populations including cancer patients, individuals undergoing cardiac procedures, and those experiencing dental care~\cite{frontiers2025musictherapy}. Optimal intervention parameters identified through systematic review suggest 30-minute sessions delivered daily for 2--4 weeks~\cite{frontiers2025musicwellbeing}. Neurological applications demonstrate similarly strong evidence: vocal music listening enhances poststroke verbal memory recovery more effectively than instrumental music or audiobooks~\cite{wiley2024neurologic}, while singing stimulates gait and motor performance in Parkinson's disease~\cite{wiley2024neurologic}.

However, access to professional music therapy remains severely constrained. The World Health Organization estimates that 75\% of individuals with mental disorders in low- and middle-income countries receive no treatment, with therapist scarcity and cost barriers limiting availability even in high-income nations. Recent IoT-based music therapy systems have begun addressing this gap: the IoT-MT platform deployed in Karachi monitored physiological parameters in 500 elderly participants and automatically delivered therapeutic music when readings indicated elevated anxiety~\cite{iotmt2025elderly}. Such systems demonstrate remote intervention feasibility but require specialized sensor hardware and dedicated infrastructure.

Browser-based synthesis could extend therapeutic music engagement to populations with only commodity devices. While our system cannot replace therapist-delivered treatment for moderate-to-severe conditions, it could serve as a complementary tool for mild symptom management and stress reduction. The active engagement afforded by voice-controlled synthesis may provide advantages over passive listening, as active music-making recruits attentional and motor networks shown to amplify therapeutic effects. The achieved sub-60ms latency maintains the immediate feedback and sense of control that music therapy literature identifies as important for therapeutic engagement.

Clinical validation would require controlled studies measuring psychological outcomes using standardized instruments alongside physiological markers. Integration of heart rate variability monitoring via smartphone photoplethysmography could provide objective outcome measures complementing self-reported states. Such research could investigate whether brief synthesis-based sessions produce measurable reductions in anxiety or stress comparable to validated music therapy protocols.

The privacy-preserving architecture proves particularly relevant for healthcare contexts. HIPAA and similar regulations impose stringent requirements on health data handling, with audio recordings of patients presenting sensitive information requiring protection. Client-side processing ensures vocal data never leaves the user's device, eliminating data breach risks associated with cloud-based services. This characteristic enables deployment in clinical settings without complex compliance reviews that cloud services would necessitate.

\subsection{Educational and Developmental Contexts}

Music education stands to benefit substantially from zero-installation tools that eliminate the IT support overhead currently impeding deployment of music technology in schools. Teachers could direct students to web URLs providing immediate access to expressive instruments without software installation, permission requests, or compatibility concerns. This accessibility particularly benefits under-resourced educational settings where dedicated music technology budgets are unavailable.

Beyond creative education, schools increasingly recognize mental health support as essential to their mission yet face severe resource constraints limiting access to counselors. Music-based interventions demonstrate efficacy for adolescent anxiety and depression~\cite{sciencedirect2025adolescent}, with technology-mediated delivery potentially achieving superior engagement in youth populations. Integration into wellness programs could provide students with self-directed stress management tools accessible during study periods or designated wellness time.

\subsection{Accessibility and Inclusive Design}

Voice-based control provides an alternative input modality for individuals unable to operate traditional instruments requiring manual dexterity. While voice control does not replicate the full expressive range of keyboard or string instruments, it may enable musical participation previously foreclosed by physical limitations. Neurologic music therapy research documents therapeutic benefits of musical engagement for diverse disability populations, suggesting accessible interaction methods could serve both creative and rehabilitative purposes.

Current limitations constrain inclusive access: users without vocal capacity, those with hearing impairments affecting pitch perception, and individuals with cognitive impairments affecting interface navigation face barriers. Future development should apply universal design principles to maximize inclusivity, potentially incorporating alternative input modalities beyond voice and providing configurable interface complexity.

\subsection{Research Methodology and Dissemination}

Browser-based deployment enables rapid prototyping and evaluation of audio algorithms with immediate access to realistic input through built-in microphones. Researchers can share working demonstrations through URLs rather than requiring evaluators to configure development environments, potentially accelerating the research feedback cycle and broadening participation in audio technology research. This characteristic proved valuable during our own development: preliminary versions were tested by colleagues across institutions without distribution complexity that native applications would entail.
